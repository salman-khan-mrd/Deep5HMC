{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# 5-fold cross validation with independent datasets \n",
    "import numpy\n",
    "from numpy import loadtxt\n",
    "from tensorflow import keras\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from tensorflow.keras import layers\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import Adam as opt\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fix random seed for reproducibility\n",
    "dataset = numpy.loadtxt(r\"E:\\MS DATA\\feature\\Hybrid_Features.csv\", delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = dataset[:,0:319]\n",
    "Y = dataset[:,320]\n",
    "cvscores=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# create folds\n",
    "folds = list(KFold(n_splits=5, shuffle=True).split(X, Y ))\n",
    "\n",
    "for j, (train, test) in enumerate(folds):\n",
    "    model = Sequential()\n",
    "    model.add(Dropout(0.4))\n",
    "    model.add(Dense(96, kernel_initializer='he_uniform', activation='relu', input_dim=319))\n",
    "    #model.add(Dropout(0.4))\n",
    "    model.add(Dense(64, kernel_initializer='he_uniform', activation='relu'))\n",
    "   # model.add3(Dropout(0.01))\n",
    "    model.add(Dense(32, kernel_initializer='he_uniform', activation='relu'))\n",
    "   # model.add(Dropout(0.01))\n",
    "    model.add(Dense(16, kernel_initializer='he_uniform', activation='relu'))\n",
    "   # model.add(Dropout(0.01))\n",
    "    model.add(Dense(8, kernel_initializer='he_uniform', activation='relu'))\n",
    "    model.add(Dense(units=1, kernel_initializer='he_uniform', activation='sigmoid'))\n",
    "\n",
    "    opt = keras.optimizers.Adam(learning_rate=0.01)\n",
    "    model.compile(loss='binary_crossentropy', optimizer='Adam',metrics=['accuracy'])\n",
    "    \n",
    "    \n",
    "    print('\\nFold = ',j)\n",
    "    X_train_cv = X[train]\n",
    "    y_train_cv = Y[train]\n",
    "    X_test_cv = X[test]\n",
    "    y_test_cv= Y[test]\n",
    "    \n",
    "\n",
    "\t# Fit the model\n",
    "    history = model.fit(X_train_cv, y_train_cv, epochs=32, batch_size=8, verbose=0)\n",
    "# evaluate the model \n",
    "    scores = model.evaluate(X_test_cv, y_test_cv, verbose=0)\n",
    "    print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n",
    "    cvscores.append(scores[1] * 100)\n",
    "print(\"Test Accuracy\",\"%.2f%%(+/- %.2f%%)\" % (numpy.mean(cvscores), numpy.std(cvscores)))\n",
    "_, train_acc = model.evaluate(X_train_cv, y_train_cv)\n",
    "print('Train Accuracy: %.2f' % (train_acc*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, train_acc = model.evaluate(X_train_cv, y_train_cv)\n",
    "print('Train Accuracy: %.2f' % (train_acc*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Predicting the test set results   # 86.04%(+/- 14.76%)\n",
    "y_pred = model.predict(X_test_cv)\n",
    "y_pred = (y_pred>0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate accuracy\n",
    "from sklearn import metrics\n",
    "print(metrics.accuracy_score(y_test_cv, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix(y_test_cv, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extracting true_positives, false_positives, true_negatives, false_negatives\n",
    "TN, FP, FN, TP = confusion_matrix(y_test_cv, y_pred).ravel()\n",
    "print(\"True Negatives: \",TN)\n",
    "print(\"False Positives: \",FP)\n",
    "print(\"False Negatives: \",FN)\n",
    "print(\"True Positives: \",TP)\n",
    "confusion = metrics.confusion_matrix(y_test_cv, y_pred)\n",
    "print(confusion)\n",
    "#[row, column]\n",
    "TP = confusion[1, 1]\n",
    "TN = confusion[0, 0]\n",
    "FP = confusion[0, 1]\n",
    "FN = confusion[1, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Precision \n",
    "Precision = TP/(TP+FP) \n",
    "print(\"Precision {:0.2f}\".format(Precision))\n",
    "#Recall \n",
    "Recall = TP/(TP+FN) \n",
    "print(\"Recall {:0.2f}\".format(Recall))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TEST ACCURACY\n",
    "_, test_acc = model.evaluate(X_test_cv, y_test_cv, verbose=0)\n",
    "print(\"Test Accuracy\",\"%.2f%%(+/- %.2f%%)\" % (numpy.mean(cvscores), numpy.std(cvscores)))\n",
    "\n",
    "#Sensitivity\n",
    "Sensitivity = TP/(FN+TP)\n",
    "print(\"Sensitivity: {:2f}\".format(Sensitivity))\n",
    "\n",
    "#Specificity \n",
    "Specificity = TN/(TN+FP)\n",
    "print(\"Specificity {:2f}\".format(Specificity))\n",
    "\n",
    "#MCC\n",
    "from sklearn.metrics import matthews_corrcoef, confusion_matrix\n",
    "confusion_matrix(y_test_cv, y_pred)\n",
    "MCC = matthews_corrcoef(y_test_cv, y_pred)\n",
    "print(\"MCC: {:2f}\".format(MCC))\n",
    "\n",
    "#F1 Score\n",
    "f1 = (2*Precision*Recall)/(Precision + Recall)\n",
    "print(\"F1 Score: {:2f}\".format(f1))\n",
    "\n",
    "ROC = metrics.roc_auc_score(y_test_cv, y_pred)\n",
    "print(\"ROC_ACC: {:2f}\".format(ROC))\n",
    "\n",
    "#Precision \n",
    "Precision = TP/(TP+FP) \n",
    "print(\"Precision: {:2f}\".format(Precision))\n",
    "\n",
    "#Recall \n",
    "#Recall \n",
    "Recall = TP/(TP+FN) \n",
    "print(\"Recall {:2f}\".format(Recall))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scikitplot as skplt\n",
    "import matplotlib.pyplot as plt\n",
    "skplt.metrics.plot_confusion_matrix(y_test_cv, y_pred, normalize=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import pyplot\n",
    "# summarize history for loss\n",
    "# plot loss during training\n",
    "#pyplot.subplot(211)\n",
    "plt.figure(figsize=(12,8))\n",
    "pyplot.plot(history.history['loss'])\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "pyplot.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot\n",
    "# plot loss during training\n",
    "#plt.title('Loss / Mean Squared Error')\n",
    "plt.figure(figsize=(10,10))\n",
    "pyplot.plot(history.history['loss'])\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "pyplot.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "plt.figure(figsize=(10,10))\n",
    "plt.plot(history.history['accuracy'])     \n",
    "plt.xlabel('epoch')  \n",
    "plt.legend(loc='upper left')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.plot(history.history['accuracy'])      \n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('accuracy') \n",
    "plt.legend(loc='upper left')  \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "false_positive_rate, true_positive_rate, thresholds = roc_curve(Y[test], y_pred)\n",
    "roc_auc = auc(false_positive_rate, true_positive_rate)\n",
    "roc_auc\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(10,10))\n",
    "#plt.title('Receiver Operating Characteristic')\n",
    "plt.plot(false_positive_rate,true_positive_rate, color='green',label = 'PsesNC' % roc_auc)\n",
    "plt.legend(loc = 'lower right')\n",
    "#plt.plot([0, 1], [0, 1],linestyle='--')\n",
    "plt.axis('tight')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.xlabel('False Positive Rate')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.svm import SVC\n",
    "from sklearn import svm\n",
    "from sklearn import metrics\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=.33,\n",
    "                                                    random_state=0)\n",
    "rf = RandomForestClassifier(max_features=5, n_estimators=100)\n",
    "rf.fit(X_train, Y_train)\n",
    "\n",
    "nb = GaussianNB()\n",
    "nb.fit(X_train, Y_train)\n",
    "\n",
    "knn = KNeighborsClassifier()\n",
    "knn.fit(X_train, Y_train)\n",
    "\n",
    "\n",
    "\n",
    "#Create a svm Classifier\n",
    "clf = svm.SVC(kernel='linear') # Linear Kernel\n",
    "\n",
    "#Train the model using the training sets\n",
    "clf.fit(X_train, Y_train)\n",
    "\n",
    "r_probs = [0 for _ in range(len(Y_test))]\n",
    "rf_probs = rf.predict_proba(X_test)\n",
    "nb_probs = nb.predict_proba(X_test)\n",
    "knn_probs = knn.predict_proba(X_test)\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "\n",
    "rf_probs = rf_probs[:, 1]\n",
    "nb_probs = nb_probs[:, 1]\n",
    "knn_probs = knn_probs[:, 1]\n",
    "\n",
    "\n",
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "\n",
    "r_auc = roc_auc_score(Y_test, r_probs)\n",
    "rf_auc = roc_auc_score(Y_test, rf_probs)\n",
    "nb_auc = roc_auc_score(Y_test, nb_probs)\n",
    "knn_auc = roc_auc_score(Y_test, knn_probs)\n",
    "svm_auc = roc_auc_score(Y_test, y_pred)\n",
    "\n",
    "# print('Random (chance) Prediction: AUROC = %.3f' % (r_auc))\n",
    "# print('Random Forest: AUROC = %.3f' % (rf_auc))\n",
    "# print('Naive Bayes: AUROC = %.3f' % (nb_auc))\n",
    "# print('K Nearest Neighbors: AUROC = %.3f' % (knn_auc))\n",
    "# print('SVM: AUROC = %.3f' % (svm_auc))\n",
    "\n",
    "# # Model Accuracy: how often is the classifier correct?\n",
    "# print('SVM: AUROC = %.3f' % metrics.accuracy_score(Y_test, y_pred))\n",
    "#svm_fpr, svm_tpr, _ = roc_auc_score(Y_test, y_pred)\n",
    "r_fpr, r_tpr, _ = roc_curve(Y_test, r_probs)\n",
    "rf_fpr, rf_tpr, _ = roc_curve(Y_test, rf_probs)\n",
    "nb_fpr, nb_tpr, _ = roc_curve(Y_test, nb_probs)\n",
    "knn_fpr, knn_tpr, _ = roc_curve(Y_test, knn_probs)\n",
    "fpr, tpr, thresholds = metrics.roc_curve(Y_test, y_pred)\n",
    "plt.plot(fpr, tpr)\n",
    "\n",
    "plt.plot(false_positive_rate,true_positive_rate,marker='.', color='green',label='NB') \n",
    "plt.plot(nb_fpr, nb_tpr, marker='.', color='red', label='RF')\n",
    "plt.plot(knn_fpr, knn_tpr, marker='.', color='blue', label='DNN')\n",
    "plt.plot(fpr, tpr, marker='.', color='pink', label='SVM')\n",
    "plt.plot(rf_fpr, rf_tpr, marker='.', color='yellow', label='SVM')\n",
    "plt.plot([0, 1], [0, 1],linestyle='--')\n",
    "\n",
    "# Title\n",
    "#plt.title('ROC Plot')\n",
    "# Axis labels\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "# Show legend\n",
    "plt.legend() # \n",
    "# Show plot\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
